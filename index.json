[
{
	"uri": "/ds/ml/",
	"title": "Ml",
	"tags": [],
	"description": "",
	"content": "Machine Learning\n"
},
{
	"uri": "/dev/misc/vim_useful/",
	"title": "Vim Cheatsheet",
	"tags": [],
	"description": "",
	"content": " Vim cheatsheet Move Faster Move to previous word (b) Move to end of line ($) Move to end of word (e) Move to beginning of line (0) Move down one line (j) Move left one character (h) Move to last line of screen (L) Move up one line (k) Move to middle line of screen (M) Move to first line of screen (H) Move right one character (l) Move to beginning of word (b) Move to next word (w)\n"
},
{
	"uri": "/me/",
	"title": "me",
	"tags": [],
	"description": "",
	"content": " Goal for 2019 I am here -\u0026gt; semi-automation -\u0026gt; full automation -\u0026gt; Intelligence\nJPG (Javascript, Python, Go) developer.  Javascript : Learning how to build a website and other web knowleage. Python : My main language. Data Science + Machine Learning Go : For dev-ops and some tasks which require high performance.  Machine Learning practioner.  General Data Science: numpy, pandas, scipy Machine Learning : scikit-learn Deep Learning : Tensorflow, Pytorch Reinforcement Learning :  Dev-ops.  Linux Docker K8s Vim  "
},
{
	"uri": "/dev/misc/ssh_useful/",
	"title": "SSH Bestpractice",
	"tags": [],
	"description": "",
	"content": " SSH reference1\nreference2\nGenerate a key ssh-keygen -t rsa -b 4096 -C \u0026quot;your_email@gmail.com\u0026quot;  Give a proper name for the key, or else, it would overwrite the id_rsa file\nAdd a key ssh-add -K ~/.ssh/id_rsa_my_key  For Mac, you need to add -k\n# Use the following command to check the result ssh-add -l  Config "
},
{
	"uri": "/dev/algorithm/",
	"title": "Algorithm",
	"tags": [],
	"description": "",
	"content": "Algorithm placeholder\n"
},
{
	"uri": "/ds/",
	"title": "Datascience",
	"tags": [],
	"description": "",
	"content": "Record Data Science Learning\n"
},
{
	"uri": "/dev/web/",
	"title": "Webtech",
	"tags": [],
	"description": "",
	"content": "Placeholder for web tech learning chapter\n javascript react html + css  "
},
{
	"uri": "/dev/",
	"title": "developer",
	"tags": [],
	"description": "",
	"content": "Placeholder for developer Chapter\n"
},
{
	"uri": "/dev/misc/raspberry_pi_3_nas/",
	"title": "Raspberry Pi 3",
	"tags": [],
	"description": "",
	"content": " Raspberry Pi 3 NAS [TOC]\nMount hard driver Create NAS Enable Time machine "
},
{
	"uri": "/dev/misc/common_commands_useful/",
	"title": "Most-used Linux CLI",
	"tags": [],
	"description": "",
	"content": " File operations Compare two directory diff --brief -r dir1/ dir2/\nIf you want to see differences for files that may not exist in either directory diff --brief -Nr dir1/ dir2\n##Commands\nxargs Delete all files with a .backup extension. -print0 on find uses a null character to split the files, and -0 changes the delimiter to the null character (useful if there\u0026rsquo;s whitespace in filenames): find . -name '*.backup' -print0 | xargs -0 rm -v\nDevops Find the top 20 high memory usage processes # Mem is the 4th field of \u0026quot;ps aux\u0026quot; ps -aux | sort -rnk 4 | head -20  If I want to the the top 20 high CPU usgae processes, I can run the following:\n# CPU is the 3rd field of \u0026quot;ps aux\u0026quot; ps -aux | sort -rnk 3 | head -20  "
},
{
	"uri": "/cloud/",
	"title": "Cloud",
	"tags": [],
	"description": "",
	"content": "Cloud Computing chapter placeholder\n"
},
{
	"uri": "/dev/misc/cli_improved/",
	"title": "Better CLI tools",
	"tags": [],
	"description": "",
	"content": " Better cli tools Ignore alias \\cat # ignore aliases named \u0026quot;cat\u0026quot; - explanation: https://stackoverflow.com/a/16506263/22617 command cat # ignore functions and aliases  bat \u0026gt; cat bat\nComparing with cat, bat also offers highlighting, paging, line numbers and git integration.\nInstallation command:\nyaourt -S bat  prettyping \u0026gt; ping fzf \u0026gt; ctrl + r In addition to searching through the history, fzf can also preview and open files, which is what I\u0026rsquo;ve done in the video below:\nalias preview=\u0026quot;fzf --preview 'bat --color \\\u0026quot;always\\\u0026quot; {}'\u0026quot; # add support for ctrl+o to open selected file in VS Code export FZF_DEFAULT_OPTS=\u0026quot;--bind='ctrl-o:execute(code {})+abort'\u0026quot;  htop \u0026gt; top  P sort by CPU - M sort by memory usage - F4 filter processes by string (to narrow to just \u0026ldquo;node\u0026rdquo; for instance) - space mark a single process so I can watch if the process is spiking ## diff-so-fancy \u0026gt; diff ## fd \u0026gt; find ## tldr \u0026gt; man ## ack || ag \u0026gt; grep  "
},
{
	"uri": "/dev/misc/cli_best_practice/",
	"title": "Build CLI Best practice",
	"tags": [],
	"description": "",
	"content": " CLI best practice reference\n[TOC]\n12 factors  Great help is essential Prefer flags to args What version am I on? Mind the streams Handle things going wrong Be fancy! Prompt if you can Use tables Be speedy Encourage contributions Be clear about subcommands Follow XDG-spec  "
},
{
	"uri": "/dev/misc/10_basic_commands_for_linux_performance_detection/",
	"title": "Check Linux Performance CLI",
	"tags": [],
	"description": "",
	"content": " 1. uptime uptime 20:25:38 up 7 min, 3 users, load average: 0.24, 0.45, 0.27 #该命令可以大致的看出计算机的整体负载情况，load average后的数字分别表示计算机在1min、5min、15min内的平均负载。  2. dmesg | tail dmesg | tail [ 51.424401] wlp5s0: RX AssocResp from 38:4c:90:8f:8c:85 (capab=0x831 status=0 aid=3) [ 51.426461] wlp5s0: associated [ 51.429463] ath: EEPROM regdomain: 0x8348 [ 51.429463] ath: EEPROM indicates we should expect a country code [ 51.429464] ath: doing EEPROM country-\u0026gt;regdmn map search [ 51.429465] ath: country maps to regdmn code: 0x3a [ 51.429465] ath: Country alpha2 being used: US [ 51.429466] ath: Regpair used: 0x3a [ 51.429466] ath: regdomain 0x8348 dynamically updated by country IE [ 51.679984] IPv6: ADDRCONF(NETDEV_CHANGE): wlp5s0: link becomes ready # 打印内核环形缓存区中的内容，可以用来查看一些错误；  3. vmstat 1 vmstat 1  4. mpstat -P ALL 1 "
},
{
	"uri": "/dev/misc/network-cli/",
	"title": "Basic networking commands in Linux",
	"tags": [],
	"description": "",
	"content": " Intro Linux is a powerful operating system, and networking is an essential part of servers.\nIt provides lots of built-in and useful third party tools\nConnectivity ping Sends an ICMP echo message (one packet) to a host.\nPing means a packet was sent from your machine via ICMP, and echoed at the IP level. Ping tells you if the other Host is Up.\nping -c 3 www.google.com PING www.google.com (172.217.12.132): 56 data bytes 64 bytes from 172.217.12.132: icmp_seq=0 ttl=54 time=28.390 ms 64 bytes from 172.217.12.132: icmp_seq=1 ttl=54 time=47.900 ms 64 bytes from 172.217.12.132: icmp_seq=2 ttl=54 time=24.011 ms --- www.google.com ping statistics --- 3 packets transmitted, 3 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 24.011/33.434/47.900/10.384 ms  telnet telnet connect destination host:port via a telnet protocol if connection establishes means connectivity between two hosts is working fine.\nNot available on macOS\narp Arp is used to translate IP addresses into Ethernet addresses.\narp -a  Routing Placeholder traceroute Print the route packets trace to network host.\ntraceroute blog.woos.me  dig DNS lookup utility\ndig (Domain Information Groper) is a flexible tool for interrogating DNS name servers.\ndig blog.woos.me  nslookup nslookup is a program to query Internet domain name servers.\nnslookup blog.woos.me  w Show who is logged on and what they are doing. Print user login, TTY, remote host, login time, idle time, current process.\nifup \u0026amp; ifdown tcpdump Dump traffic on a network.\n# Capture the traffic of a specific interface: tcpdump -i \u0026lt;eth0\u0026gt;  Reference basic linux networking commands\n"
},
{
	"uri": "/dev/misc/authentication-protocol/",
	"title": "Authentication Protocols",
	"tags": [],
	"description": "",
	"content": " Terminology  Token vs Cookies Json Web Tokens (JWT) OpenID connect OAuth 2 LDAP SAML Kerboes AD \u0026amp; ADFS SSO  What is OpenID OpenID Connect (OIDC) is an authentication protocol, based on the OAuth 2.0 family of specifications. It uses simple JSON Web Tokens (JWT), which you can obtain using flows conforming to the OAuth 2.0 specifications.\nWhile OAuth 2.0 is about resource access and sharing, OIDC is all about user authentication. Its purpose is to give you one login for multiple sites. Each time you need to log in to a website using OIDC, you are redirected to your OpenID site where you login, and then taken back to the website.\n"
},
{
	"uri": "/dev/cs/working-with-a-sql-database-in-go/",
	"title": "Working With a SQL Database in Go",
	"tags": [],
	"description": "",
	"content": " database/sql Go offers a clean SQL database API in its standard library database/sql package, but the specific database drivers must be installed separately.\nIt’s a smart approach because it provides a common interface that nearly every DB driver implements.\nIf you want to use MySQL, you can use https://github.com/go-sql-driver/mysql.\nIf you’re using Postgres, use https://github.com/lib/pq.\nYou just need to include the lib using import _, and the database/sql API will be configured to enable that driver:\nimport \u0026quot;database/sql\u0026quot; import _ \u0026quot;github.com/go-sql-driver/mysql\u0026quot;  Note\n import 下划线（如：import _ hello/imp）的作用：当导入一个包时，该包下的文件里所有init()函数都会被执行，然而，有些时候我们并不需要把整个包都导入进来，仅仅是是希望它执行init()函数而已。这个时候就可以使用 import _ 引用该包。即使用 import _ 包 只是引用该包，仅仅是为了调用init()函数，所以无法通过包名来调用包中的其他函数。\n Reference Working With a SQL Database in Go\nWhy we import SQL drives as the blank identifier (_) in Go\n"
},
{
	"uri": "/dev/misc/setup-postgresql/",
	"title": "How to Setup Postgresql",
	"tags": [],
	"description": "",
	"content": " Installation Ubuntu sudo apt update sudo apt install postgresql postgresql-contrib  Manjaro pacman -S postgresql  Authentication New installation has a postgres user by default\npostgres has no DB password set on Ubuntu by default.\nYou can login to that account only by using the postgres OS user account\nsudo -u postgres psql  Change the password psql\u0026gt; ALTER USER \u0026lt;username\u0026gt; PASSWORD 'newpassword';  Rule of thumb:\nYOU SHOULD NEVER EVER SET A PASSWORD FOR THE POSTGRES USER.\nAllow remote connection By default PostgreSQL is configured to be bound to \u0026ldquo;localhost\u0026rdquo;\nCheck the port (5432) Run following command to see what is the port 5432 bound to\nnetstat -nlt Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:5432 0.0.0.0:* LISTEN  As we can see above port 5432 is bound to 127.0.0.1. It means any attempt to connect to the postgresql server from outside the machine will be refused. We can try hitting the port 5432 by using telnet.\ntelnet \u0026lt;remote-server-ip-address\u0026gt; 5432  Configuring postgresql.conf Open /etc/postgresql/postgresql.conf\nReplace the\nlisten_addresses = 'localhost'  with\nlisten_addresses = '*'  And restart the PostgreSQL service\nConfiguring pg_hba.conf Open pg_hba.conf and add following entry at the very end\n# First entry for IPv4 and second entry for IPv6 host all all 0.0.0.0/0 md5 host all all ::/0 md5  Configuring firewall In real world, you might have firewall running\nOpen port 5432 for TCP\nMore configuration PostgreSQL can be configured and tuned through a series of configuration files.\nUnderstanding postgresql.conf Most global configuration settings are stored in postgresql.conf, which is created automatically when you install PostgreSQL.\nTune Authentication Options through pg_hba.conf The pg_hba.conf file handles the default authentication options for client connections to the database.\nPG CLI Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nInstallation macOS\nbrew install pgcli  Others\npip install pgcli  Usage pgcli postgres://\u0026lt;username\u0026gt;:\u0026lt;passw0rd\u0026gt;@\u0026lt;host\u0026gt;:5432/\u0026lt;database\u0026gt;  Reference Install Postgresql in manjaro\nConfigure PostgreSQL to allow remote connection\nConfigure PostgreSQL by Linode\n"
},
{
	"uri": "/dev/misc/httpie/",
	"title": "Intro to Httpie",
	"tags": [],
	"description": "",
	"content": " What is HTTPie HTTPie Official Website\nHTTPie is a command line HTTP client with intuitive UI, JSON support, syntax highlighting, wget-like downloads, plugins and more.\nInstallation macOS brew install httpie\nLinux pacman -S httpie\nUsage Synopsis:\nhttp [flags] [METHOD] URL [ITEM [ITEM]]  "
},
{
	"uri": "/dev/misc/dockerfile_intro/",
	"title": "Dockerfile_intro",
	"tags": [],
	"description": "",
	"content": " 01 关于Dockerfile 在Docker中创建镜像最常用的方式，就是使用Dockerfile。Dockerfile是一个Docker镜像的描述文件，我们可以理解成火箭发射的A、B、C、D…的步骤。Dockerfile其内部包含了一条条的指令，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n一个Dockerfile的示例如下所示：\n#基于centos镜像 FROM centos #维护人的信息 MAINTAINER The CentOS Project #安装httpd软件包 RUN yum -y update RUN yum -y install httpd #开启80端口 EXPOSE 80 #复制网站首页文件至镜像中web站点下 ADD index.html /var/www/html/index.html #复制该脚本至镜像中，并修改其权限 ADD run.sh /run.sh RUN chmod 775 /run.sh #当启动容器时执行的脚本文件 CMD [\u0026quot;/run.sh\u0026quot;]  由上可知，Dockerfile结构大致分为四个部分：\n 基础镜像信息 维护者信息 镜像操作指令 容器启动时执行指令。  Dockerfile每行支持一条指令，每条指令可带多个参数，支持使用以#号开头的注释。下面会对上面使用到的一些常用指令做一些介绍。\n02 Dockerfile常用指令 2.1 FROM 指明构建的新镜像是来自于哪个基础镜像，例如：\nFROM centos:6\n2.2 MAINTAINER 指明镜像维护着及其联系方式（一般是邮箱地址），例如：\nMAINTAINER example example@gmail.com\n2.3 RUN 构建镜像时运行的Shell命令，例如：\nRUN [\u0026quot;yum\u0026quot;, \u0026quot;install\u0026quot;, \u0026quot;httpd\u0026quot;] RUN yum install httpd  又如，我们在使用微软官方ASP.NET Core Runtime镜像时往往会加上以下RUN命令，弥补无法在默认镜像下使用Drawing相关接口的缺憾：\nFROM microsoft/dotnet:2.2.1-aspnetcore-runtime RUN apt-get update RUN apt-get install -y libgdiplus RUN apt-get install -y libc6-dev RUN ln -s /usr/lib/libgdiplus.so /lib/x86_64-linux-gnu/libgdiplus.so  2.4 CMD 启动容器时执行的Shell命令，例如：\nCMD [\u0026quot;-C\u0026quot;, \u0026quot;/start.sh\u0026quot;] CMD [\u0026quot;/usr/sbin/sshd\u0026quot;, \u0026quot;-D\u0026quot;] CMD /usr/sbin/sshd -D  2.5 EXPOSE 声明容器运行的服务端口，例如： EXPOSE 80 443\n2.6 ENV 设置环境内环境变量，例如：\nENV MYSQL_ROOT_PASSWORD 123456 ENV JAVA_HOME /usr/local/jdk1.8.0_45  2.7 ADD 拷贝文件或目录到镜像中，例如：\nADD \u0026lt;src\u0026gt;...\u0026lt;dest\u0026gt; ADD html.tar.gz /var/www/html ADD https://xxx.com/html.tar.gz /var/www/html  　PS：如果是URL或压缩包，会自动下载或自动解压。\n2.8 COPY 拷贝文件或目录到镜像中，用法同ADD，只是不支持自动下载和解压，例如：\nCOPY ./start.sh /start.sh  2.9 ENTRYPOINT 启动容器时执行的Shell命令，同CMD类似，只是由ENTRYPOINT启动的程序不会被docker run命令行指定的参数所覆盖，而且，这些命令行参数会被当作参数传递给ENTRYPOINT指定指定的程序，例如：\nENTRYPOINT [\u0026quot;/bin/bash\u0026quot;, \u0026quot;-C\u0026quot;, \u0026quot;/start.sh\u0026quot;] ENTRYPOINT /bin/bash -C '/start.sh'  　PS：Dockerfile文件中也可以存在多个ENTRYPOINT指令，但仅有最后一个会生效。\n2.10 VOLUME 指定容器挂载点到宿主机自动生成的目录或其他容器，例如：\nVOLUME [\u0026quot;/var/lib/mysql\u0026quot;]  PS：一般不会在Dockerfile中用到，更常见的还是在docker run的时候指定-v数据卷。\n2.11 USER 为RUN、CMD和ENTRYPOINT执行Shell命令指定运行用户，例如：\nUSER \u0026lt;user\u0026gt;[:\u0026lt;usergroup\u0026gt;] USER \u0026lt;UID\u0026gt;[:\u0026lt;UID\u0026gt;] USER example  2.12 WORKDIR 为RUN、CMD、ENTRYPOINT以及COPY和AND设置工作目录，例如：\nWORKDIR /data  2.13 HEALTHCHECK 告诉Docker如何测试容器以检查它是否仍在工作，即健康检查，例如：\nHEALTHCHECK --interval=5m --timeout=3s --retries=3 \\ CMD curl -f http:/localhost/ || exit 1  　其中，一些选项的说明：\n\u0026ndash;interval=DURATION (default: 30s)：每隔多长时间探测一次，默认30秒\n\u0026ndash; timeout= DURATION (default: 30s)：服务响应超时时长，默认30秒\n\u0026ndash;start-period= DURATION (default: 0s)：服务启动多久后开始探测，默认0秒\n\u0026ndash;retries=N (default: 3)：认为检测失败几次为宕机，默认3次\n　一些返回值的说明：\n0：容器成功是健康的，随时可以使用\n1：不健康的容器无法正常工作\n2：保留不使用此退出代码\n2.14 ARG 在构建镜像时，指定一些参数，例如：\nFROM centos:6 ARG user # ARG user=root USER $user  　这时，我们在docker build时可以带上自定义参数user了，如下所示：\ndocker build \u0026ndash;build-arg user=example Dockerfile .\n03 综合Dockerfile案例 下面是一个Java Web应用的镜像Dockerfile，综合使用到了上述介绍中最常用的几个命令：\nFROM centos:7 MAINTANIER www.edisonchou.com ADD jdk-8u45-linux-x64.tar.gz /usr/local ENV JAVA_HOME /usr/local/jdk1.8.0_45 ADD apache-tomcat-8.0.46.tar.gz /usr/local COPY server.xml /usr/local/apache-tomcat-8.0.46/conf RUN rm -f /usr/local/*.tar.gz WORKDIR /usr/local/apache-tomcat-8.0.46 EXPOSE 8080 ENTRYPOINT [\u0026quot;./bin/catalina.sh\u0026quot;, \u0026quot;run\u0026quot;] 有了Dockerfile，就可以创建镜像了： docker build -t edc-tomcat:v1 . 最后，可以通过以下命令创建容器： docker run -itd --name=tomcate -p 8080:8080 \\ -v /app/webapps/:/usr/local/apache-tomcat-8.0.46/webapps/ \\ edc-tomcat:v1  "
},
{
	"uri": "/cloud/elasticsearch-query-intro/",
	"title": "Elasticsearch Query Intro",
	"tags": [],
	"description": "",
	"content": " Elasticsearch Communicate with Elasticsearch: this is done by issuing HTTP requests against the REST API. Elastic is started by default in port 9200.\nElasticsearch Query DSL The query DSL is a flexible, expressive search language that Elasticsearch uses to expose most of the power of Lucene through a simple JSON interface. It is what you should be using to write your queries in production. It makes your queries more flexible, more precise, easier to read, and easier to debug. 1\nReference 23 Useful Elasticsearch Example Queries\nA Practical Introduction to Elasticsearch\n Query DSL Intro [return]   "
},
{
	"uri": "/cloud/elastic-stacks-setup/",
	"title": "Elastic Stacks Setup",
	"tags": [],
	"description": "",
	"content": " What is Elastic Stacks The Elastic Stack — formerly known as the ELK Stack — is a collection of open-source software produced by Elastic which allows you to search, analyze, and visualize logs generated from any source in any format, a practice known as centralized logging. Centralized logging can be very useful when attempting to identify problems with your servers or applications, as it allows you to search through all of your logs in a single place. It\u0026rsquo;s also useful because it allows you to identify issues that span multiple servers by correlating their logs during a specific time frame.\nThe Elastic Stack has four main components:\n Elasticsearch: a distributed RESTful search engine which stores all of the collected data. Use Memory Logstash: the data processing component of the Elastic Stack which sends incoming data to Elasticsearch. Kibana: a web interface for searching and visualizing logs. Beats: lightweight, single-purpose data shippers that can send data from hundreds or thousands of machines to either Logstash or Elasticsearch.  Install Elastic Stacks TODO\nTest the installation TODO\nWork with Windows Installing and configuring the winlogbeat Test-NetConnection elastic-instance-ip -Port 5044  Reference Install ELK on ubuntu 18.04\n"
},
{
	"uri": "/cloud/logstash-101/",
	"title": "Intro to logstash",
	"tags": [],
	"description": "",
	"content": " Manage the Logstash plugin Logstash has a rich collection of input, filter, codec and output plugins.\nPlugins are available as self-contained packages called gems and hosted on RubyGems.org.\nThe plugin manager accessed via bin/logstash-plugin script is used to manage the lifecycle of plugins in your Logstash deployment. You can install, remove and upgrade plugins using the Command Line Interface (CLI) invocations described below.\n# on ubuntu 18.04, you can find the logstash installation in /usr/share/logstash # list all installed plugins bin/logstash-plugin list  Reference Logstash plugin\n"
},
{
	"uri": "/dev/misc/time-explain/",
	"title": "Time Command Explaination",
	"tags": [],
	"description": "",
	"content": " Use time # interactive time command # Results # Use it in file \\time -o time.log command # Results  Reference Linux time command\n"
},
{
	"uri": "/dev/web/restful/",
	"title": "RESTful",
	"tags": [],
	"description": "",
	"content": " What is REST REST, or Representational State Transfer is another way of implementing a Web Service. It is the ideology that everything is a resource. Every resource is identified by a unique uniform resource indicator (URI). An important point to note with REST is that it is stateless. This means that when you access the REST web service to retrieve data, you receive the most current state of the resource (ie. most recent data available). You might go for a quick walk, come back, send another request to the web service, and now you have different data. REST APIs usually take advantage of HTTP, however it’s also possible to use it with other protocols.\nBenefits of REST  Lightweight Scalable - Supports large numbers of requests Reliable - No single point of failure  Reference Build rest api in 5 mins\n"
},
{
	"uri": "/dev/misc/shell_compatiability/",
	"title": "Shells",
	"tags": [],
	"description": "",
	"content": " Shell types  sh or Bourne Shell: the original shell still used on UNIX systems and in UNIX-related environments. This is the basic shell, a small program with few features. While this is not the standard shell, it is still available on every Linux system for compatibility with UNIX programs. bash or Bourne Again shell: the standard GNU shell, intuitive and flexible. Probably most advisable for beginning users while being at the same time a powerful tool for the advanced and professional user. On Linux, bash is the standard shell for common users. This shell is a so-called superset of the Bourne shell, a set of add-ons and plug-ins. This means that the Bourne Again shell is compatible with the Bourne shell: commands that work in sh, also work in bash. However, the reverse is not always the case. All examples and exercises in this book use bash. csh or C shell: the syntax of this shell resembles that of the C programming language. Sometimes asked for by programmers. tcsh or TENEX C shell: a superset of the common C shell, enhancing user-friendliness and speed. That is why some also call it the Turbo C shell. ksh or the Korn shell: sometimes appreciated by people with a UNIX background. A superset of the Bourne shell; with standard configuration a nightmare for beginning users.  Differing features The table below shows major differences between the standard shell (sh), Bourne Again SHell (bash), Korn shell (ksh) and the C shell (csh)\n   sh bash ksh csh Meaning/Action     $ $ $ % Default user prompt    \u0026gt;| \u0026gt;| \u0026gt;! Force redirection   \u0026gt; file 2\u0026gt;\u0026amp;1 \u0026amp;\u0026gt; file or \u0026gt; file 2\u0026gt;\u0026amp;1 \u0026gt; file 2\u0026gt;\u0026amp;1 \u0026gt;\u0026amp; file Redirect stdout and stderr to file    { }  { } Expand elements in list   command command or $(command) $(command) command Substitute output of enclosed command   $HOME $HOME $HOME $home Home directory    ~ ~ ~ Home directory symbol    ~+, ~-, dirs ~+, ~- =-, =N Access directory stack   var=value VAR=value var=value set var=value Variable assignment   export var export VAR=value export var=val setenv var val Set environment variable    ${nnnn} ${nn}  More than 9 arguments can be referenced   \u0026ldquo;$@\u0026ldquo; \u0026ldquo;$@\u0026ldquo; \u0026ldquo;$@\u0026ldquo;  All arguments as separate words   $# $# $# $#argv Number of arguments   $? $? $? $status Exit status of the most recently executed command   $! $! $!  PID of most recently backgrounded process   $- $- $-  Current options   . file source file or . file . file source file Read commands in file    alias x=\u0026lsquo;y\u0026rsquo; alias x=y alias x y Name x stands for command y   case case case switch or case Choose alternatives   done done done end End a loop statement   esac esac esac endsw End case or switch   exit n exit n exit n exit (expr) Exit with a status   for/do for/do for/do foreach Loop through variables    set -f, set -o nullglob|dotglob|nocaseglob|noglob  noglob Ignore substitution characters for filename generation   hash hash alias -t hashstat Display hashed commands (tracked aliases)   hash cmds hash cmds alias -t cmds rehash Remember command locations   hash -r hash -r  unhash Forget command locations    history history history List previous commands    ArrowUp+Enter or !! r !! Redo previous command    !str r str !str Redo last command that starts with \u0026ldquo;str\u0026rdquo;    !cmd:s/x/y/ r x=y cmd !cmd:s/x/y/ Replace \u0026ldquo;x\u0026rdquo; with \u0026ldquo;y\u0026rdquo; in most recent command starting with \u0026ldquo;cmd\u0026rdquo;, then execute.   if [ $i -eq 5 ] if [ $i -eq 5 ] if ((i==5)) if ($i==5) Sample condition test   fi fi fi endif End if statement   ulimit ulimit ulimit limit Set resource limits   pwd pwd pwd dirs Print working directory   read read read $\u0026lt; Read from terminal   trap 2 trap 2 trap 2 onintr Ignore interrupts    unalias unalias unalias Remove aliases   until until until  Begin until loop   while/do while/do while/do while Begin while loop    Reference Differing features\nBash guide for beginners\n"
},
{
	"uri": "/dev/misc/docker_beginner/",
	"title": "Docker: From Zero to Hero",
	"tags": [],
	"description": "",
	"content": " Dockerfile # Dockerfile FROM node:latest WORKDIR /app COPY . . RUN npm install EXPOSE 3000 ENTRYPOINT [\u0026quot;node\u0026quot;, \u0026quot;app.js\u0026quot;]   FROM, this is us selecting an OS image from Docker Hub. Docker Hub is a global repository that contains images that we can pull down locally. In our case we are choosing an image based on Ubuntu that has Node.js installed, it’s called node. We also specify that we want the latest version of it, by using the following tag :latest WORKDIR, this simply means we set a working directory. This is a way to set up for what is to happen later, in the next command below COPY, here we copy the files from the directory we are standing into the directory specified by our WORKDIR command RUN, this runs a command in the terminal, in our case we are installing all the libraries we need to build our Node.js express application EXPOSE, this means we are opening up a port, it is through this port that we communicate with our container ENTRYPOINT, this is where we should state how we start up our application, the commands need to be specified as an array so the array [“node”, “app.js”] will be translated to the node app.js in the terminal  First things first, let’s create our image with the following command:\ndocker build -t adenosinew/node:latest .  Reference Docker - from the beginning, part I Docker - from the beginning, part II\n"
},
{
	"uri": "/dev/misc/linux-cheatsheet/",
	"title": "Bash Cheatsheet on macOS",
	"tags": [],
	"description": "",
	"content": " Overview Jure did a very impressive job on the Linux cheatsheet.1 It covered generic linux and some part of ubuntu and Debian distribution.\nSince I am a user of macOS, a lot of shortcuts and commands are not available on macOS\u0026rsquo;s platform, thus I am going to write my own cheatsheet.\nYou can also see this one as a best practice of using terminal on macOS.\nTerminal shortcuts [macOS iTerm2] Package management Homebrew is a widely used package management tool on macOS platform.\nReference  Comprehensive Linux Cheatsheet [return]   "
},
{
	"uri": "/dev/misc/work-with-different-shells/",
	"title": "Work with Different Shells",
	"tags": [],
	"description": "",
	"content": " There are several shells such as bash, sh, ksh, zsh, fish and many other lesser known shells available on Linux.\nBash (/bin/bash) is a popular shell on most if not all Linux systems, and it’s normally the default shell for user accounts.\nWhen creating user accounts with the useradd or adduser utilities, the --shell flag can be used to specify the name of a user’s login shell other than that specified in the respective configuration files.\nList All Shells cat /etc/shells # /etc/shells: valid login shells /bin/sh /bin/dash /bin/bash /bin/rbash /usr/bin/tmux /usr/bin/screen /bin/csh /bin/tcsh /usr/bin/tcsh  Method 1. usermod utility User’s account details, stored in the /etc/passwd file.\nIf I want to change the default shell of ubuntu from bash to tcsh\nusermod --shell /bin/tcsh ubuntu # Check the /etc/passwd to validate the change grep ubuntu /etc/passwd  Method 2. chsh chsh -s /bin/tcsh ubuntu  Method 3. Edit the /etc/passwd Simply open the /etc/passwd file using any of your favorite command line text editors and change a specific users shell.\nReference Change default shell\n"
},
{
	"uri": "/ds/sqlalchemy/",
	"title": "Sqlalchemy",
	"tags": [],
	"description": "",
	"content": " Sqlalchemy Reflection Metadata "
},
{
	"uri": "/dev/misc/terraform_useful/",
	"title": "Terraform Best Practice",
	"tags": [],
	"description": "",
	"content": " What is Terraform Terraform official site\nTerraform is a super useful tool developed by HashiCorp, the main idea of terraform is \u0026ldquo;INFRASTRUCTURE AS CODE\u0026rdquo;.\nTo me, the most attracting feature Cloud Computing is scalability. Infrastructure in Cloud is inifinite. But there are some downsides of traditional cloud library.\nFor AWS, I\u0026rsquo;ve tried awscli, boto3 and go sdk. For a small test, they are easy to use. But for a big test, there are too many logics and dependencies inside. Not to mention, there are a lot cloud vendors, such as Azure, GCP and more.\nInstall and configure Terraform Terraform is a single binary, the installation of Terraform is easy. All you need to do is download the binary and export the path to $PATH.\nFor Mac user, simply run: (I\u0026rsquo;ve notice the brew version of terraform is slower than HashiCorp\u0026rsquo;s binary)\nbrew install terraform # Add autocomplete terraform -install-autocomplete  *.tf files *.tfvars files and variable Reference Terraform Variables\n"
},
{
	"uri": "/dev/cs/python_platform/",
	"title": "Python Platform module",
	"tags": [],
	"description": "",
	"content": " platform Access to underlying platform’s identifying data # Get Architecture, The function relies on the system’s file command to do the actual work. print(platform.architecture()) # \u0026gt;\u0026gt; ('64bit', '') # More reliable way to get the architecture of current interpreter is_64bits = sys.maxsize \u0026gt; 2**32 # \u0026gt;\u0026gt; True # Returns the machine type, e.g. 'i386'. An empty string is returned if the value cannot be determined. print(platform.machine()) # \u0026gt;\u0026gt; x86_64 # Returns the computer’s network name (may not be fully qualified!). An empty string is returned if the value cannot be determined. print(platform.node()) # \u0026gt;\u0026gt; Adenosine-MacBook-Pro.local # print(platform.platform()) # \u0026gt;\u0026gt; Darwin-18.2.0-x86_64-i386-64bit # Returns a single string identifying the underlying platform with as much useful information as possible. print(platform.processor()) # \u0026gt;\u0026gt; i386  "
},
{
	"uri": "/dev/misc/setup_mysql/",
	"title": "Install MySQL server on CentOS",
	"tags": [],
	"description": "",
	"content": " My laptop is running out of storage, since I have a VPS running as VPN server. I found it would be a good practice to setup a MySQL server in remote.\nBTW, if you still have AWS free tier, AWS RDS service is super easy and stable.\nInstall the MySQL server I am not a big fan of ubuntu, on my local, I am using Manjaro. And for remote server, my first choice is always CentOS.\nMySQL must be installed from the community repository.\n Download and add the repository, then update.\nwget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm yum update  Install MySQL as usual and start the service. During installation, you will be asked if you want to accept the results from the .rpm file’s GPG verification. If no error or mismatch occurs, enter y.\nsudo yum install mysql-server sudo systemctl start mysqld   Optimize MySQL server You will be given the choice to change the MySQL root password, remove anonymous user accounts, disable root logins outside of localhost, and remove test databases. It is recommended that you answer yes to these options. You can read more about the script in the MySQL Reference Manual.\nsudo mysql_secure_installation  MySQL Tuner is a Perl script that connects to a running instance of MySQL and provides configuration recommendations based on workload.\nwget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/mysqltuner.pl perl ./mysqltuner.pl  Create User and database Basic User interaction List all users SELECT User, Host FROM mysql.user; +------------------+-----------+ | User | Host | +------------------+-----------+ | root | localhost | | root | demohost | | root | 127.0.0.1 | | debian-sys-maint | localhost | | | % | +------------------+-----------+  List all databases and tables SHOW databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+  Choose one database and list all tables\nuse mysql; show tables; +---------------------------+ | Tables_in_mysql | +---------------------------+ | columns_priv | | db | ... | time_zone_name | | time_zone_transition | | time_zone_transition_type | | user | +---------------------------+ 32 rows in set (0.00 sec)  Using MySQL From localhost Root Login mysql -u root -p  Create new database and user Remote access Add permission to public access GRANT ALL PRIVILEGES ON *.* TO 'username'@'%' IDENTIFIED BY 'password' WITH GRANT OPTION; FLUSH PRIVILEGES;  Connect to the database from CLI mysql -u {username} -h {remote server ip or name} -P {port} -D {DB name} -p{password}  Notes No space between -p and {password} , better leave the password for blank and enter the password from the prompt.\nMySQL Better CLI mycli is a command line interface for MySQL, MariaDB, and Percona with auto-completion and syntax highlighting.\nbrew install mycli mycli mysql://my_user@my_host.com:3306/my_database  Reference Install mysql on centos Linode\nAllow remote connection to mysql\nBasic User interaction\nAdd user and grant privileges\n"
},
{
	"uri": "/dev/misc/active_directory/",
	"title": "Active_directory",
	"tags": [],
	"description": "",
	"content": " Windows Active Directory What is Active Directory Domain Controller (DC) A server of Active Directory service\nSchema (database)\n User account Computer account  Schema extentable\nUser Account Contains\n username password Email and etc.  Computer Account Contains\n computer name UID and etc.  Groups OU (Organization Unit) "
},
{
	"uri": "/ds/numpy_fancy_indexing_and_viewing/",
	"title": "Fancy indexing, view and copy",
	"tags": [],
	"description": "",
	"content": " Normal Numpy Indexing Numpy arrays using:\n simple indices (e.g., arr[0]), slices (e.g., arr[:5]) Boolean masks (e.g., arr[arr \u0026gt; 0])  Numpy Fancy Indexing Fancy indexing is conceptually simple: it means passing an array of indices to access multiple array elements at once.\nimport numpy as np rand = np.random.RandomState(42) x = rand.randint(100, size=10) print(x) # Output # [51 92 14 71 60 20 82 86 74 74] # Normal indexing [x[3], x[7], x[2]] # [71, 86, 14] # Fancy indexing ind = [3, 7, 4] x[ind] # [71, 86, 14]  When using fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed:\nind = np.array([[3, 7], [4, 5]]) x[ind] # array([[71, 86], # [60, 20]])  Fancy indexing also works in multiple dimensions. Consider the following array:\nX = np.arange(12).reshape((3, 4)) X # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]])  Like with standard indexing, the first index refers to the row, and the second to the column:\nrow = np.array([0, 1, 2]) col = np.array([2, 1, 3]) X[row, col] # array([ 2, 5, 11])  Notice that the first value in the result is X[0, 2], the second is X[1, 1], and the third is X[2, 3]. The pairing of indices in fancy indexing follows all the broadcasting rules that were mentioned in Computation on Arrays: Broadcasting.\n It is always important to remember with fancy indexing that the return value reflects the broadcasted shape of the indices, rather than the shape of the array being indexed.\n Numpy view and copy With a view, it’s like you are viewing the original (base) array. The view is actually part of the original array even though it looks like you’re working with something else. These are analogous to shallow copies in Python.\nCopies are separate objects from the original array, though right after copying the two look the same. These are analogous to deep copies in Python.\nIn short, view == shallow copy, copy == deep copy\narray.base\n###When you get a view vs a copy\nSo when do you get a view and when do you get a copy?\n    View Copy     Slices Indexing, e.g. Z[0,:] Fancy indexing, e.g. Z[[0],:]   Changing dtype / W = Z.as_type(np.float32)   Converting to 1D array Z.ravel() Z.flatten()    Reference Fancy index\nview vs copy\n"
},
{
	"uri": "/ds/pytorch/",
	"title": "Pytorch Learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/dev/misc/systemd_mount/",
	"title": "Mount a filesystem",
	"tags": [],
	"description": "",
	"content": " tldr description of mount tldr mount mount Provides access to an entire filesystem in one directory. - Show all mounted filesystems: mount - Mount a device to a directory: mount -t filesystem_type path/to/device_file path/to/target_directory - Mount a CD-ROM device (with the filetype ISO9660) to /cdrom (readonly): mount -t iso9660 -o ro /dev/cdrom /cdrom - Mount all the filesystem defined in /etc/fstab: mount -a - Mount a specific filesystem described in /etc/fstab (e.g. \u0026quot;/dev/sda1 /my_drive ext2 defaults 0 2\u0026quot;): mount /my_drive  Use CLI to mount a filesystem For instance, mount a AWS EFS filesystem to your EC2 instance /files folder\nmount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 fs-XXXXXX.efs.us-west-2.amazonaws.com:/ /files  Use systemd to mount a filesystem Requirement: the filename of the mount file should be the same as the Where field.\n# files.mount [Unit] Description=Mount The File System After=network.target [Mount] What=fs-XXXXX.efs.us-west-2.amazonaws.com:/ Where=/files Type=nfs Options=auto [Install] WantedBy=multi-user.target  Start the service\n# In ubuntu, move it to /etc/systemd/system/ cp files.mount /etc/systemd/system/ systemctl daemon-reload systemctl start files.mount  Mount as a specific user If I want to mount the filesystem as the ubuntu user, specify it in the Options field.\n[Unit] Description=Mount The File System After=network.target [Mount] What=fs-XXXXXX.efs.us-east-2.amazonaws.com:/ Where=/files Type=nfs Options=nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,uid=$(id -u ubuntu),gid=$(id -g ubuntu) [Install] WantedBy=multi-user.target  Mount as a normal user In above section, uid and gid is not available for sudo user. Reason unknown.\nTODO: Find out why uid and gid are invalid options for sudo and ubuntu user.\nFound this option can be used for sudo user:\nnosuid\nDisables the set-user-identifier and set-group-identifier bits. This prevents remote users from gaining higher privileges by running a setuid program.\n[Unit] Description=Mount The File System After=network.target [Mount] What=fs-XXXXXX.efs.us-east-2.amazonaws.com:/ Where=/files Type=nfs Options=nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,nosuid [Install] WantedBy=multi-user.target  Using /etc/fstab We can mount the remote NFS shares automatically at boot by adding them to /etc/fstab file on the client.\n# /etc/fstab entry . . . 203.0.113.0:/var/nfs/general /nfs/general nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0 203.0.113.0:/home /nfs/home nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0  Reference Redhat NFS client reference\nRedhat NFS Mount reference\nHow to setup a nfs mount\n"
},
{
	"uri": "/dev/misc/",
	"title": "Linux",
	"tags": [],
	"description": "",
	"content": "Placeholder for linux Chapter\n"
},
{
	"uri": "/me/better_develpoer/",
	"title": "10X Develpoer",
	"tags": [],
	"description": "",
	"content": " Goal for 2019 Update on Aug 13 2019\n Golang  Get to know Golang Build APIs with Golang  Cloud certification  Get to know AWS certification Prepare for a middle level AWS test  Web development  HTTP basic HTML + CSS Get to know Javascript  Python  Leetcode Practice (~200) Data Science Recap (Numpy + Pandas)   Popular Topics for developer Database Web Development Automation APIs APP development Front End Web Scraper Machine Learning  Useful suggestions  The human brain is a complex machine and it\u0026rsquo;s a result of thousands of years of human evolution. Our world has evolved very fast. For the brain, being such an old machine, working in today\u0026rsquo;s complex and distracted world is very hard. But if we use our brain as our ancestors did, we can use most of it. The following steps will help you achieve that.\n challenges: - complex - distraction\n You have to be an action-based person to learn better.\n Reference Use the full power of Your Brain to be a Better Developer \n"
},
{
	"uri": "/dev/misc/docker_useful/",
	"title": "Useful commands for docker",
	"tags": [],
	"description": "",
	"content": " References Top 10 Docker CLI commands you can\u0026rsquo;t live without\nLists running containers # show all containers docker ps -a # only list ids docker ps -q  Get base image from Docker Hub docker pull  Build docker images from a Dockerfile and a \u0026ldquo;context\u0026rdquo; docker build -t container_label .  Run a docker container based on an image Top 10 options for docker run\ndocker run my_image -it bash   --detach, -d --entrypoint --env, -e or --env-file --ip --name --publish, -p | --publish-all, -P --rm --tty, -t Command use with the option --interactive, -i --volume, -v --workdir, -w  Display the logs of a container docker logs --follow my_container  List the volumes, which are the preferred mechanism for persisting data generated by and used by Docker containers docker volume ls  Removes one or more containers docker rm my_container  Removes one or more images docker rmi my_image  Stops one or more containers docker stop my_container # docker kill my_container  Kill all running containers docker kill $(docker ps -q)  Delete all stopped containers docker rm $(docker ps -a -q)  Delete all images docker rmi $(docker images -q)  Update all images docker images |grep -v REPOSITORY|awk '{print $1}'|xargs -L1 docker pull  Enter a running docker container docker exec -it [container-id] bash  Copy file from docker container to host docker cp \u0026lt;containerId\u0026gt;:/file/path/within/container /host/path/target  List ports defined on a container docker port CONTAINER [Private_port[/protocol]] # Example $ docker port proxy 80/tcp -\u0026gt; 0.0.0.0:80 $ docker port proxy 80 0.0.0.0:80  A vs B Dokerfile EXPOSE vs publish EXPOSE:\nWhen writing your dockerfiles, the instruction EXPOSE tells Docker the running container listens on specific network ports.\nEXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;/\u0026lt;protocol\u0026gt;...]  Or within docker run command:\ndocker run --expose=1234 my_container  Publish ports and map them to the host\nThere are several flags you can use when using the docker run command to publish a container\u0026rsquo;s ports outside of the container\u0026rsquo;s network and map them to the host machine\u0026rsquo;s ports.\ndocker run -p 80:80/tcp -p 80:80/udp my_app # To publish all the ports you define in your Dockerfile with EXPOSE and bind them to the host machine, you can use -P flag docker run -P my_app  "
},
{
	"uri": "/dev/misc/git_useful/",
	"title": "Useful Git commands",
	"tags": [],
	"description": "",
	"content": " Overview:\nThere are tons of tutorial of git. For those basic concepts, I am going to link to some good tutorial. This one is focusing on some real life use cases, what if you encounter these errors or what if I want to do this\u0026hellip;\nBasic Git commands git fetch The git fetch command downloads commits, files, and refs from a remote repository into your local repo. Fetching is what you do when you want to see what everybody else has been working on.\ngit merge TODO\ngit log TODO\nGit use case (snippets) Create a new branch based on a arbitrary branch TODO\nCreate a new branch based on a commit git log # Get a commit id git checkout -b \u0026lt;new_branch_name\u0026gt; \u0026lt;commit_id\u0026gt;  Update a branch (feature) with different branch (develop) git checkout develop git pull git checkout feature git merge develop # or git rebase develop  The difference between with git rebase is that rebase keep all commits history from your branch, and that is important if your partial commits have a lot of content that can be interesting to keep. This option is obligatory in some teams.\nRemove grey submodule git rm --cached \u0026lt;folder_name\u0026gt; git add . git commit -m \u0026quot;\u0026lt;your_message\u0026gt;\u0026quot; git push --all  Fix conflicts TODO\nReference Atlassian git fetch\n"
},
{
	"uri": "/ds/commands_for_data_scientist/",
	"title": "Command line tricks for data scientist",
	"tags": [],
	"description": "",
	"content": " reference\nICONV HEAD TR WC SPLIT SORT \u0026amp; UNIQ CUT PASTE JOIN GREP  SED AWK  "
},
{
	"uri": "/",
	"title": "adenosinew",
	"tags": [],
	"description": "",
	"content": " About Me and this site My name is Adenosine. And this is my site to record my learning and thinking process.\nI am using a *star* as my profile picture because star is some word play of my Chinese name.  4 years ago, I was a undergraduate student majored in the Plant Genetics. 2 years ago, I was a Applied Math and Statistics master student. And now, I am a data scientist and developer in Cloud Computing. And I found switching between different area is fantastic experience, and it also give me different perspectives to look at questions.\nAs a engineer, I always believe \u0026ldquo;quantity\u0026rdquo; is the only factor that make difference. When you stack up ideas and work, everything changes. Such as nucleic acid sequence, only needs A,C,G,T, it can generate billions of forms of life. And in the world of computer, 1 and 0 carry enormous information in a light speed.\n\u0026ldquo;How could I make difference?\u0026rdquo; I asked myself. That\u0026rsquo;s why I started publish my thinking and works on this site. My goal is update this site once a week at the beginning. When I am more familiar with english writing and get more time, I am going to update it more often.\nAdenosine Feb 10th 2019\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]